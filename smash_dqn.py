# -*- coding: utf-8 -*-
"""Smash-DQN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DNOxbJJRo8_gVMHkmiGxUxgJAlAIoBSI
"""

# %%
import gym, gym_mupen64plus, torch, cv2, time
import numpy as np
import torch.nn as nn
from torch.distributions import Categorical
import torch.nn.functional as F

import matplotlib.pyplot as plt
import matplotlib.animation as animation

from collections import namedtuple
import pickle

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

def process_state(state):
    state = cv2.resize(state, (64, 64))
    state = np.transpose(state)
    return torch.Tensor(state)

def map_action_space(action_i):
    action = [0] * 8
    if action_i == 0:
        pass
    elif action_i == 1:
        action[0] = 127
    elif action_i == 2:
        action[0] = -128
    elif action_i == 3:
        action[1] = 127
    elif action_i == 4:
        action[1] = -128
    elif action_i == 5:
        action[0] = 127
        action[1] = 127
    elif action_i == 6:
        action[0] = 127
        action[1] = -128
    elif action_i == 7:
        action[0] = -128
        action[1] = 127
    elif action_i == 8:
        action[0] = -128
        action[1] = -128
    else:
        action[action_i - 7] = 1
    return action

def save_video(buffer, epoch):
  deep_frames = []
  for f in buffer.records[-args.max_steps:]:
    deep_frames += [f.state[0].T]
  plt.figure(figsize=(deep_frames[0].shape[1] / 72.0, deep_frames[0].shape[0] / 72.0), dpi = 72)                                          
  patch = plt.imshow(deep_frames[0])
  plt.axis('off')
  animate = lambda i: patch.set_data(deep_frames[i])
  ani = animation.FuncAnimation(plt.gcf(), animate, frames=len
  (deep_frames), interval = 50)

  Writer = animation.writers['ffmpeg']
  writer = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)
  ani.save('training_%i.mp4' % epoch, writer=writer)

def save_training(model, buffer):
    torch.save(model.state_dict(), './DQN.pth')
    #with open("buffer.pyobj", "w") as f:
    #    pickle.dump(buffer, f)

def load_training(model):
    model.load_state_dict(torch.load("./DQN.pth"))
    #with open("buffer.pyobj", "r") as f:
    #    buffer = pickle.load(f)
    return buffer

def save_stats(args):
    with open("statistics.pyobj", "a") as f:
        pickle.dump(args, f)


# %%
Feedback = namedtuple('feedback', ['state', 'reward', 'done', 'info'])
Record = namedtuple('record', ['action', 'state', 'reward', 'done'])
Transition = namedtuple('transition', ['state', 'action', 'next_state', 'reward', 'done'])

class ExpBuffer:
  def __init__(self, max_size = 30000):
    self.max_size = max_size
    self.records = []
    self.state_shape = None
  
  def add_record(self, action, state, reward, done):
    if len(self.records) == self.max_size: self.records.pop(0)
    self.state_shape = state.shape
    self.records.append(Record(action, state, reward, done))
  
  def add_state(self, state):
    self.add_record(None, state, None, False)

  def sample(self, batch_size, device='cpu'):
    if len(self.records) <= 1: raise Error('Sampling before buffer is filled') 
    states = torch.zeros(batch_size, *buffer.state_shape)
    actions = torch.zeros(batch_size, dtype=torch.long)
    next_states = torch.zeros(batch_size, *buffer.state_shape)
    rewards = torch.zeros(batch_size)
    done = torch.zeros(batch_size, dtype=torch.bool)

    for i in range(batch_size):
      while True:
        idx = np.random.randint(0, len(self.records))
        if idx != 0 and self.records[idx].action is not None: break
      record = self.records[idx]
      states[i] = self.records[idx-1].state
      actions[i] = record.action
      rewards[i] = record.reward
      if record.done: done[i] = True
      else: next_states[i] = record.state

    return Transition(states.to(device), actions.to(device), 
                      next_states.to(device), rewards.to(device), 
                      done.to(device))
    
    
    return Transition(self.records[idx-1].state, *self.records[idx])
    
  
  def __len__(self): return len(self.records)

def get_DQN(action_dim): 
  return nn.Sequential(
    nn.Conv2d(3, 16, 7, 2, 3), # 32, 32
    nn.MaxPool2d(2), # 16, 16
    nn.ReLU(),
    nn.Conv2d(16, 64, 3, 2, 1), # 8, 8
    nn.MaxPool2d(2), # 4, 4
    nn.Flatten(), # 16
    nn.Linear(4 * 4 * 64, action_dim)
  )

def get_action(action_dim, state):
  if np.random.rand() < args.epsilon:
    action = np.random.randint(action_dim)
  else:
    model.eval()
    Q = model(state.unsqueeze(0).to(device))
    action = torch.max(Q, 1)[1].item()
  return action

def optimize():
  # train using past transitions
  model.train()
  optimizer.zero_grad()
  # randomly sample a batch of transitions from buffer
  batch = buffer.sample(args.batch_size, device)

  # predicted value of next state
  Qs = model(batch.state)
  outputs = Qs[np.arange(args.batch_size), batch.action]
  # estimated value of next state
  next_Qs = torch.max(model(batch.next_state), 1)[0]
  targets = args.gamma * batch.reward + next_Qs * ~batch.done

  # calculate loss
  loss = loss_func(outputs, targets)
  loss.backward()
  optimizer.step()

  # log loss
  #args.losses.append(loss)

class Args: pass
args = Args()
args.lr = 0.003
args.l2reg = 0.0003
args.episodes = 100
args.max_steps = 800
args.epsilon = 0.1
args.batch_size = 32
args.gamma = 0.99
args.losses = []
args.actions = []
args.rewards = []
args.reset = False

buffer = ExpBuffer()

env = gym.make('Smash-dk-v0')
action_dim = 15

model = get_DQN(action_dim)
if not args.reset:
    buffer = load_training(model)

model.to(device)
optimizer = torch.optim.Adam(model.parameters(), args.lr)
# Huber Loss: equivalent to MSE when difference is small, but less punishing
# when the difference is large, resilient to outliers
loss_func = F.smooth_l1_loss

# %%
def train():
    for episode_idx in range (1, args.episodes+1):
        state = env.reset()
        state = process_state(state)
        buffer.add_state(state)
        score = 0

        for t in range(args.max_steps):
            # calculate next action using epsilon-greedy
            action = get_action(action_dim, state)

            # step
            state, reward, done, info = env.step(map_action_space(action))
            state = process_state(state)
            buffer.add_record(action, state, reward, done)
            score += reward
            
            optimize()

            # log r, a
            #args.rewards.append(reward)
            #args.actions.append(action)

        if episode_idx % 10 == 0:
            save_training(model, buffer)
            save_stats(args)
            save_video(buffer, episode_idx)

        
        print('episode %d, score %f' % (episode_idx, score))

#train()